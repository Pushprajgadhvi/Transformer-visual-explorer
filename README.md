# ğŸš€ Transformer Architecture â€“ Interactive Visual Explorer

<p align="center">
  <b>An interactive platform to visualize and understand the internal mechanics of the Transformer architecture used in modern LLMs such as GPT and BERT.</b>
</p>

<p align="center">
  ğŸŒ <a href="https://transformer-lime.vercel.app/" target="_blank"><b>Live Demo</b></a>
</p>

---

## ğŸ“– Overview

Transformer Visual Explorer is a **browser-based interactive learning platform** that demonstrates how the Transformer model works internally.

Instead of only presenting theory, this platform converts **mathematical operations into interactive visual simulations**, allowing users to explore attention mechanisms, positional encoding, and encoderâ€“decoder interactions step-by-step.

It helps learners build **deep intuition about attention**, which is the core mechanism behind modern Large Language Models.

---

## âœ¨ Key Features

- Interactive Self-Attention visualization  
- Multi-Head Attention simulation  
- Positional Encoding heatmap (Sineâ€“Cosine formulation)  
- Encoder architecture breakdown  
- Decoder architecture breakdown  
- Masked Self-Attention explanation  
- Cross-Attention conceptual visualization  
- Mathematical formulas with implementation logic  
- Fully client-side execution (no backend required)  

---

## ğŸ§  Concepts Implemented

This platform demonstrates the complete Transformer pipeline:

- Token Embeddings  
- Positional Encoding  
- Scaled Dot-Product Attention  
- Multi-Head Attention  
- Masked Self-Attention  
- Cross Attention  
- Residual Connections  
- Layer Normalization  
- Position-wise Feed Forward Network  
- Encoder Stack  
- Decoder Stack  

---

## ğŸ§© Transformer Architecture Flow

Input Tokens
â†“
Embedding Layer
â†“
Positional Encoding
â†“
Encoder Stack
â”œâ”€â”€ Multi-Head Self Attention
â”œâ”€â”€ Add & LayerNorm
â”œâ”€â”€ Feed Forward Network
â””â”€â”€ Add & LayerNorm
â†“
Decoder Stack
â”œâ”€â”€ Masked Self Attention
â”œâ”€â”€ Cross Attention
â”œâ”€â”€ Feed Forward Network
â””â”€â”€ LayerNorm
â†“
Output Probabilities

---

## ğŸ”¬ Core Attention Formula

Scaled Dot-Product Attention:

Attention(Q, K, V) = softmax((QKáµ€) / âˆšdâ‚–) V


Implementation includes:

- Linear projections for Query, Key, and Value  
- Dot-product similarity computation  
- Scaling by âˆšdâ‚– for stability  
- Softmax normalization  
- Weighted aggregation with Value vectors  
- Multi-head splitting and concatenation  

---

## ğŸ“¸ Screenshots
<img width="1919" height="897" alt="image" src="https://github.com/user-attachments/assets/30c25c7a-0e6a-4fd5-b888-9164a67ea7a5" />

<img width="1919" height="901" alt="image" src="https://github.com/user-attachments/assets/4e438528-f271-47e1-89f2-47d083f37c23" />

<img width="1919" height="904" alt="image" src="https://github.com/user-attachments/assets/4cf3e816-3b47-4c9d-8a8e-f60865c2ff47" />

<img width="1919" height="907" alt="image" src="https://github.com/user-attachments/assets/8462f57f-bb92-42f5-9b02-cc7f6c4158c4" />


---

## ğŸ›  Tech Stack

| Layer | Technology |
|------|------------|
| Frontend | HTML5, CSS3, JavaScript |
| Visualization | Canvas / SVG / DOM |
| Architecture Logic | Custom JavaScript |
| Deployment | Vercel |
| Hosting | Static Web App |

---

## ğŸ“‚ Project Structure
Transformer-visual-explorer/
â”‚
â”œâ”€â”€ index.html
â”œâ”€â”€ style.css
â”œâ”€â”€ script.js
â”œâ”€â”€ assets/
â”œâ”€â”€ screenshots/
â””â”€â”€ README.md


---

## ğŸŒ Deployment

Live Application:  
https://transformer-lime.vercel.app/

Hosted using Vercel with:

- Automatic GitHub deployment  
- HTTPS enabled  
- Global CDN  
- Fast static serving  

---

## ğŸ¯ Purpose

Transformers involve complex matrix operations and abstract mathematical concepts.  
This project makes those concepts **visual, interactive, and intuitive**, helping learners move from theory to real understanding.

---

## ğŸš€ Future Improvements

- Real text input attention visualization  
- Token-level attention heatmaps  
- PyTorch integration  
- Training visualization  
- Interactive parameter tuning  

---

## ğŸ‘¨â€ğŸ’» Author

**Pushpraj Gadhvi**

GitHub:  
https://github.com/Pushprajgadhvi/

---



